{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.metrics.distance import jaccard_distance, edit_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_w_pairs</th>\n",
       "      <th>seq_label</th>\n",
       "      <th>pair_label</th>\n",
       "      <th>context</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_299</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_299_259_0</td>\n",
       "      <td>ADILABAD : TRS condemns arrests March 11 , 201...</td>\n",
       "      <td>ADILABAD : TRS condemns arrests March 11 , 201...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_09_B_24</td>\n",
       "      <td>2505</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_09_B_24_2505_0</td>\n",
       "      <td>However , local villagers resisted the searche...</td>\n",
       "      <td>However , &lt;ARG1&gt;local villagers resisted the s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_02_109</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_02_109_291_0</td>\n",
       "      <td>In 2009 , riots broke out in the capital , Uru...</td>\n",
       "      <td>In 2009 , &lt;ARG0&gt;riots broke out in the capital...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_257</td>\n",
       "      <td>2109</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_257_2109_0</td>\n",
       "      <td>The 60 - page charge sheet , filed by National...</td>\n",
       "      <td>The 60 - page charge sheet , filed by National...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_05_223</td>\n",
       "      <td>1332</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_05_223_1332_0</td>\n",
       "      <td>TREATMENT Residents of Ravensmead on the Cape ...</td>\n",
       "      <td>TREATMENT &lt;ARG1&gt;Residents of Ravensmead on the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus         doc_id  sent_id  eg_id                     index  \\\n",
       "0    cnc   train_05_299      259      0    cnc_train_05_299_259_0   \n",
       "1    cnc  train_09_B_24     2505      0  cnc_train_09_B_24_2505_0   \n",
       "2    cnc   train_02_109      291      0    cnc_train_02_109_291_0   \n",
       "3    cnc   train_05_257     2109      0   cnc_train_05_257_2109_0   \n",
       "4    cnc   train_05_223     1332      0   cnc_train_05_223_1332_0   \n",
       "\n",
       "                                                text  \\\n",
       "0  ADILABAD : TRS condemns arrests March 11 , 201...   \n",
       "1  However , local villagers resisted the searche...   \n",
       "2  In 2009 , riots broke out in the capital , Uru...   \n",
       "3  The 60 - page charge sheet , filed by National...   \n",
       "4  TREATMENT Residents of Ravensmead on the Cape ...   \n",
       "\n",
       "                                        text_w_pairs  seq_label  pair_label  \\\n",
       "0  ADILABAD : TRS condemns arrests March 11 , 201...          1           1   \n",
       "1  However , <ARG1>local villagers resisted the s...          1           1   \n",
       "2  In 2009 , <ARG0>riots broke out in the capital...          1           1   \n",
       "3  The 60 - page charge sheet , filed by National...          1           1   \n",
       "4  TREATMENT <ARG1>Residents of Ravensmead on the...          1           1   \n",
       "\n",
       "   context  num_sents  \n",
       "0      NaN          1  \n",
       "1      NaN          1  \n",
       "2      NaN          1  \n",
       "3      NaN          1  \n",
       "4      NaN          1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/Users/ananyathomas/Documents/GitHub/case_shared_task_3/codefiles/subtask_2_data/train_subtask2.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>eg_id</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>causal_text_w_pairs</th>\n",
       "      <th>num_rs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_0</td>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_0_892_0</td>\n",
       "      <td>The State alleged they hacked Sabata Petros Ch...</td>\n",
       "      <td>['The State alleged &lt;ARG1&gt;they hacked Sabata P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_1</td>\n",
       "      <td>2714</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_1_2714_0</td>\n",
       "      <td>Chale was allegedly chased by a group of about...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_10</td>\n",
       "      <td>2619</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_10_2619_0</td>\n",
       "      <td>The farmworkers ' strike resumed on Tuesday wh...</td>\n",
       "      <td>[\"&lt;ARG1&gt;The farmworkers ' strike resumed on Tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_100</td>\n",
       "      <td>2680</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_100_2680_0</td>\n",
       "      <td>Demonstrators have filed for a permit to hold ...</td>\n",
       "      <td>['&lt;ARG1&gt;Demonstrators have filed for a permit&lt;...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnc</td>\n",
       "      <td>train_01_101</td>\n",
       "      <td>3090</td>\n",
       "      <td>0</td>\n",
       "      <td>cnc_train_01_101_3090_0</td>\n",
       "      <td>Footage of the attack , which included a pregn...</td>\n",
       "      <td>['&lt;ARG0&gt;Footage of the attack , which included...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus        doc_id  sent_id  eg_id                    index  \\\n",
       "0    cnc    train_01_0      892      0     cnc_train_01_0_892_0   \n",
       "1    cnc    train_01_1     2714      0    cnc_train_01_1_2714_0   \n",
       "2    cnc   train_01_10     2619      0   cnc_train_01_10_2619_0   \n",
       "3    cnc  train_01_100     2680      0  cnc_train_01_100_2680_0   \n",
       "4    cnc  train_01_101     3090      0  cnc_train_01_101_3090_0   \n",
       "\n",
       "                                                text  \\\n",
       "0  The State alleged they hacked Sabata Petros Ch...   \n",
       "1  Chale was allegedly chased by a group of about...   \n",
       "2  The farmworkers ' strike resumed on Tuesday wh...   \n",
       "3  Demonstrators have filed for a permit to hold ...   \n",
       "4  Footage of the attack , which included a pregn...   \n",
       "\n",
       "                                 causal_text_w_pairs  num_rs  \n",
       "0  ['The State alleged <ARG1>they hacked Sabata P...       1  \n",
       "1                                                 []       0  \n",
       "2  [\"<ARG1>The farmworkers ' strike resumed on Tu...       1  \n",
       "3  ['<ARG1>Demonstrators have filed for a permit<...       2  \n",
       "4  ['<ARG0>Footage of the attack , which included...       2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grouped_df = pd.read_csv('/Users/ananyathomas/Documents/GitHub/case_shared_task_3/codefiles/subtask_2_data/train_subtask2_grouped.csv')\n",
    "train_grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus             0\n",
      "doc_id             0\n",
      "sent_id            0\n",
      "eg_id              0\n",
      "index              0\n",
      "text               0\n",
      "text_w_pairs       0\n",
      "seq_label          0\n",
      "pair_label         0\n",
      "context         2257\n",
      "num_sents          0\n",
      "dtype: int64\n",
      "(2257, 11)\n",
      "corpus           object\n",
      "doc_id           object\n",
      "sent_id           int64\n",
      "eg_id             int64\n",
      "index            object\n",
      "text             object\n",
      "text_w_pairs     object\n",
      "seq_label         int64\n",
      "pair_label        int64\n",
      "context         float64\n",
      "num_sents         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isna().sum())\n",
    "print(train_df.shape)\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.5.12)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from num2words) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import string\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    'n\\'t':'not',\n",
    "    'won\\'t':'will not',\n",
    "    'can\\'t':'can not',\n",
    "    'i\\'m':'i am',\n",
    "    'you\\'re':'you are',\n",
    "    'he\\'s':'he is',\n",
    "    'she\\'s':'she is',\n",
    "    'it\\'s':'it is',\n",
    "    'we\\'re':'we are',\n",
    "    'they\\'re':'they are',\n",
    "    'i\\'ve':'i have',\n",
    "    'you\\'ve':'you have',\n",
    "    'we\\'ve':'we have',\n",
    "    'NaMo':'Narendra Modi',\n",
    "    'PMO':'Prime Minister Office',\n",
    "    'PM':'Prime Minister',\n",
    "    'BJP':'Bharatiya Janata Party',\n",
    "    'INC':'Indian National Congress',\n",
    "    'AAP':'Aam Aadmi Party',\n",
    "    'J&K':'Jammu and Kashmir',\n",
    "    'NHAI':'National Highways Authority of India',\n",
    "    'CPI':'Communist Party of India',\n",
    "    'CPI(M)':'Communist Party of India (Marxist)',\n",
    "    'CPI-M':'Communist Party of India (Marxist)',\n",
    "    'HQs':'Headquarters',\n",
    "    'NGO':'Non Governmental Organization',\n",
    "    'NGOs':'Non Governmental Organizations',\n",
    "    'TRS':'Telangana Rashtra Samithi',\n",
    "    'IST':'Indian Standard Time',\n",
    "    'DA': 'Dearness Allowance',\n",
    "    'MP':'Member of Parliament',\n",
    "    'DGP':\"Director General of Police\",\n",
    "    'WCED': \"World Commission on Environment and Development\",\n",
    "    'JCB': \"Joseph Cyril Bamford\",\n",
    "    'CPI': \"Consumer Price Index\",\n",
    "    'ADM': \"Additional District Magistrate\",\n",
    "    'PIL': \"Public Interest Litigation\",\n",
    "    'PTI': \"Press Trust of India\",\n",
    "    'TNCC': \"Tamil Nadu Congress Committee\",\n",
    "    'JAC': \"Joint Action Committee\",\n",
    "    'CPM': \"Cost per thousand\",\n",
    "    'KCR': \"Kalvakuntla Chandrashekar Rao\",\n",
    "    'NMMU': \"Nelson Mandela Metropolitan University\", \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_links(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    x = [word for word in tokens if not urlparse(word).scheme]\n",
    "    text = ' '.join(x)\n",
    "    return text\n",
    "\n",
    "def contractions_handling(text):\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "def replacing_abbr(text, dictry):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    x = [dictry[word] if word in dictry.keys() else word for word in tokens]\n",
    "    text = ' '.join(x)\n",
    "    return text\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def removing_stopwords(text):\n",
    "    #tokenizer = TweetTokenizer()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = word_tokenize(text)\n",
    "    #tokens = tokenizer.tokenize(text)\n",
    "    x = [word for word in tokens if not word.lower() in stop_words]\n",
    "    x = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            x.append(w)\n",
    "    return text\n",
    "\n",
    "def convert_num_words(text):\n",
    "    re_results = re.findall('(\\d+(st|nd|rd|th))', text)\n",
    "    for entire_result, suffix in re_results:\n",
    "        num = int(entire_result[:-len(suffix)])\n",
    "        entire_result = \" \" + entire_result\n",
    "        num_word = num2words.num2words(num, ordinal=True)\n",
    "        text = text.replace(entire_result, \" \" + num_word)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [num2words.num2words(word) if word.isdigit() else word for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def removing_char_less_3(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/ananyathomas/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/ananyathomas/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: replacing_abbr(x, abbreviations))\n\u001b[1;32m      4\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: removing_punctuations(x))\n\u001b[0;32m----> 5\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: removing_stopwords(x))\n\u001b[1;32m      6\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: convert_num_words(x))\n\u001b[1;32m      7\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: removing_char_less_3(x))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1157\u001b[0m             values,\n\u001b[1;32m   1158\u001b[0m             f,\n\u001b[1;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: replacing_abbr(x, abbreviations))\n\u001b[1;32m      4\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: removing_punctuations(x))\n\u001b[0;32m----> 5\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: removing_stopwords(x))\n\u001b[1;32m      6\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: convert_num_words(x))\n\u001b[1;32m      7\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: removing_char_less_3(x))\n",
      "Cell \u001b[0;32mIn[38], line 26\u001b[0m, in \u001b[0;36mremoving_stopwords\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremoving_stopwords\u001b[39m(text):\n\u001b[1;32m     25\u001b[0m     \u001b[39m#tokenizer = TweetTokenizer()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)) \n\u001b[1;32m     27\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m     28\u001b[0m     \u001b[39m#tokens = tokenizer.tokenize(text)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/ananyathomas/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.10/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: removing_links(x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: contractions_handling(x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: replacing_abbr(x, abbreviations))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: removing_punctuations(x))\n",
    "#train_df['text'] = train_df['text'].apply(lambda x: removing_stopwords(x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: convert_num_words(x))\n",
    "train_df['text'] = train_df['text'].apply(lambda x: removing_char_less_3(x))\n",
    "\n",
    "train_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
