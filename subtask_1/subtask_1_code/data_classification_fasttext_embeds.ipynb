{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:52:33.856743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "seed = 2000\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, MaxPooling1D, Dropout, Flatten, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "num_i = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               index                                               text  label\n",
      "0     train_01_0_892  the state alleged they hacked sabata petros ch...      1\n",
      "1    train_01_1_2714  chale was allegedly chased group about thirty ...      0\n",
      "2   train_01_10_2619  the farmworkers strike resumed tuesday when th...      1\n",
      "3  train_01_100_2680  demonstrators have filed for permit hold rally...      1\n",
      "4  train_01_101_3090  footage the attack which included pregnant wom...      1\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_data/train_subtask1_preprocessed_{}.csv'.format(num_i))\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0\n",
      " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0\n",
      " 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0\n",
      " 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0\n",
      " 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1\n",
      " 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 0 1 1 1 0]\n",
      "               index                                               text\n",
      "0    train_10_0_2136  the movement was catapulted into the headlines...\n",
      "1     train_10_1_350  several thousand protesters took the streets a...\n",
      "2   train_10_10_3104  the protest not just about saving medha life b...\n",
      "3  train_10_100_1188  hong kong baptist university protest contrite ...\n",
      "4  train_10_100_1734  published saturday three february two thousand...\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_data/dev_subtask1_preprocessed_{}.csv'.format(num_i))\n",
    "dev_df.head()\n",
    "\n",
    "dev_df_labels = pd.read_csv('/Users/nitanshjain/Documents/Projects/CASE/tanfiona CausalNewsCorpus master data-V2/dev_subtask1.csv')\n",
    "labels = dev_df_labels['label'].values\n",
    "del(dev_df_labels)\n",
    "print(labels)\n",
    "print(dev_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 2\n",
    "min_word = 2\n",
    "down_sampling = 1e-2\n",
    "\n",
    "fast_text_model_train = FastText(train_df.text.tolist(),\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers=4,\n",
    "                      sg=1)\n",
    "\n",
    "fast_text_model_dev = FastText(dev_df.text.tolist(),\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers=4,\n",
    "                      sg=1)\n",
    "\n",
    "fast_text_model_train.save(\"/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_train_text_{}\".format(num_i))\n",
    "fast_text_model_train = Word2Vec.load(\"/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_train_text_{}\".format(num_i))\n",
    "\n",
    "fast_text_model_dev.save(\"/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_dev_text_{}\".format(num_i))\n",
    "fast_text_model_dev = Word2Vec.load(\"/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_dev_text_{}\".format(num_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3075, 100)\n",
      "(340, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.065436</td>\n",
       "      <td>0.019627</td>\n",
       "      <td>-0.019218</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.005223</td>\n",
       "      <td>-0.058909</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.082406</td>\n",
       "      <td>-0.082391</td>\n",
       "      <td>-0.036890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005942</td>\n",
       "      <td>0.041117</td>\n",
       "      <td>0.036963</td>\n",
       "      <td>-0.056914</td>\n",
       "      <td>0.097453</td>\n",
       "      <td>-0.018364</td>\n",
       "      <td>-0.024337</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>-0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053786</td>\n",
       "      <td>0.018129</td>\n",
       "      <td>-0.012374</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>-0.051774</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.072879</td>\n",
       "      <td>-0.077181</td>\n",
       "      <td>-0.031455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>0.046006</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>-0.047711</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>-0.011583</td>\n",
       "      <td>-0.018929</td>\n",
       "      <td>0.051499</td>\n",
       "      <td>-0.044079</td>\n",
       "      <td>-0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.058167</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>-0.013781</td>\n",
       "      <td>0.028405</td>\n",
       "      <td>0.007915</td>\n",
       "      <td>-0.059294</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.078766</td>\n",
       "      <td>-0.081738</td>\n",
       "      <td>-0.031280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.044201</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>-0.048893</td>\n",
       "      <td>0.096150</td>\n",
       "      <td>-0.012868</td>\n",
       "      <td>-0.023155</td>\n",
       "      <td>0.046271</td>\n",
       "      <td>-0.053691</td>\n",
       "      <td>-0.010426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059094</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>-0.013480</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>-0.057071</td>\n",
       "      <td>0.008123</td>\n",
       "      <td>0.074087</td>\n",
       "      <td>-0.079854</td>\n",
       "      <td>-0.033300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002901</td>\n",
       "      <td>0.047384</td>\n",
       "      <td>0.032736</td>\n",
       "      <td>-0.049737</td>\n",
       "      <td>0.095013</td>\n",
       "      <td>-0.011365</td>\n",
       "      <td>-0.024717</td>\n",
       "      <td>0.047307</td>\n",
       "      <td>-0.045663</td>\n",
       "      <td>0.000717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.060764</td>\n",
       "      <td>0.014861</td>\n",
       "      <td>-0.018518</td>\n",
       "      <td>0.020886</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>-0.058670</td>\n",
       "      <td>0.009772</td>\n",
       "      <td>0.086035</td>\n",
       "      <td>-0.085776</td>\n",
       "      <td>-0.034804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003310</td>\n",
       "      <td>0.041735</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>-0.053423</td>\n",
       "      <td>0.097148</td>\n",
       "      <td>-0.015153</td>\n",
       "      <td>-0.017781</td>\n",
       "      <td>0.050120</td>\n",
       "      <td>-0.050465</td>\n",
       "      <td>-0.000627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \n",
       "0 -0.065436  0.019627 -0.019218  0.019246  0.005223 -0.058909  0.005642  \\\n",
       "1 -0.053786  0.018129 -0.012374  0.028447  0.006552 -0.051774  0.013088   \n",
       "2 -0.058167  0.015079 -0.013781  0.028405  0.007915 -0.059294  0.012637   \n",
       "3 -0.059094  0.018051 -0.013480  0.021814  0.008642 -0.057071  0.008123   \n",
       "4 -0.060764  0.014861 -0.018518  0.020886  0.008679 -0.058670  0.009772   \n",
       "\n",
       "          7         8         9  ...        90        91        92        93   \n",
       "0  0.082406 -0.082391 -0.036890  ... -0.005942  0.041117  0.036963 -0.056914  \\\n",
       "1  0.072879 -0.077181 -0.031455  ... -0.003560  0.046006  0.033148 -0.047711   \n",
       "2  0.078766 -0.081738 -0.031280  ...  0.002789  0.044201  0.027809 -0.048893   \n",
       "3  0.074087 -0.079854 -0.033300  ... -0.002901  0.047384  0.032736 -0.049737   \n",
       "4  0.086035 -0.085776 -0.034804  ... -0.003310  0.041735  0.034714 -0.053423   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.097453 -0.018364 -0.024337  0.046799 -0.047586 -0.001964  \n",
       "1  0.102236 -0.011583 -0.018929  0.051499 -0.044079 -0.004301  \n",
       "2  0.096150 -0.012868 -0.023155  0.046271 -0.053691 -0.010426  \n",
       "3  0.095013 -0.011365 -0.024717  0.047307 -0.045663  0.000717  \n",
       "4  0.097148 -0.015153 -0.017781  0.050120 -0.050465 -0.000627  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_file(create_file, model_file, x):\n",
    "    fast_text_model = Word2Vec.load(model_file)\n",
    "    \n",
    "    with open(create_file, 'w+') as word2vec_file:\n",
    "        for index, row in x.iterrows():\n",
    "            model_vector = (np.mean([fast_text_model.wv[token] for token in row['text']], axis=0)).tolist()\n",
    "            if index == 0:\n",
    "                header = \",\".join(str(ele) for ele in range(100))\n",
    "                word2vec_file.write(header)\n",
    "                word2vec_file.write(\"\\n\")\n",
    "            \n",
    "            if type(model_vector) is list:\n",
    "                line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "            else:\n",
    "                line1 = \",\".join([str(0) for i in range(100)])\n",
    "            word2vec_file.write(line1)\n",
    "            word2vec_file.write('\\n')\n",
    "    \n",
    "    df = pd.read_csv(create_file)\n",
    "    return df\n",
    "\n",
    "fast_text_train_filename = '/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_data/train_ft_{}.csv'.format(num_i)\n",
    "fast_text_train_model_file = '/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_train_text_{}'.format(num_i)\n",
    "fast_text_train_embeddings_df = create_file(fast_text_train_filename, fast_text_train_model_file, train_df)\n",
    "\n",
    "fast_text_train_embeddings = fast_text_train_embeddings_df.values\n",
    "\n",
    "print(fast_text_train_embeddings_df.shape)\n",
    "fast_text_train_embeddings_df.head()\n",
    "\n",
    "\n",
    "fast_text_dev_filename = '/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_data/dev_ft_{}.csv'.format(num_i)\n",
    "fast_text_dev_model_file = '/Users/nitanshjain/Documents/Projects/CASE/codefiles/subtask_1_embeddings/ft_model_dev_text_{}'.format(num_i)\n",
    "fast_text_dev_embeddings_df = create_file(fast_text_dev_filename, fast_text_dev_model_file, dev_df)\n",
    "\n",
    "fast_text_dev_embeddings = fast_text_dev_embeddings_df.values\n",
    "\n",
    "print(fast_text_dev_embeddings_df.shape)\n",
    "fast_text_dev_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0596140592694694, 1: 0.9467364532019704} 0.8934729064039408\n"
     ]
    }
   ],
   "source": [
    "x = fast_text_train_embeddings_df.values\n",
    "y = train_df['label'].values\n",
    "\n",
    "x_dev = fast_text_dev_embeddings_df.values\n",
    "y_dev = labels\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "x = mm.fit_transform(x)\n",
    "\n",
    "# Calculating Classweights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = np.unique(y),\n",
    "    y = y\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "count_0 = np.unique(y, return_counts=True)[1][0]\n",
    "count_1 = np.unique(y, return_counts=True)[1][1]\n",
    "estimate = count_0/count_1\n",
    "\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "print(class_weights, estimate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(scale_pos_weight=estimate)\n",
    "\n",
    "parameters = {\n",
    "            'objective':['binary:logistic'],\n",
    "            'learning_rate': [0.1, 0.01, 0.001, 0.0001], \n",
    "            'max_depth': [6, 7, 8],\n",
    "            'n_estimators': [1000], #number of trees, change it to 1000 for better results\n",
    "            'seed': [1337]\n",
    "        }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=cv, \n",
    "                   verbose=0, refit=True)\n",
    "\n",
    "clf.fit(x, y)\n",
    "print(clf.best_params_, clf.best_score_)\n",
    "\n",
    "y_pred = clf.predict(x_dev)\n",
    "print(classification_report(y_dev, y_pred))\n",
    "print(f1_score(y_dev, y_pred))\n",
    "print(recall_score(y_dev, y_pred))\n",
    "print(precision_score(y_dev, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_prior': None, 'fit_prior': False} 0.5681300813008131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       155\n",
      "           1       0.00      0.00      0.00       185\n",
      "\n",
      "    accuracy                           0.46       340\n",
      "   macro avg       0.23      0.50      0.31       340\n",
      "weighted avg       0.21      0.46      0.29       340\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "parameters = {\n",
    "            'fit_prior': [True, False],\n",
    "            'class_prior': [None, [0.5, 0.5], [0.6, 0.4], [0.4, 0.6]]\n",
    "        }\n",
    "\n",
    "mnb_gsc = GridSearchCV(mnb, parameters, n_jobs=5, \n",
    "                   cv=cv, \n",
    "                   verbose=0, refit=True)\n",
    "\n",
    "mnb_gsc.fit(x, y)\n",
    "print(mnb_gsc.best_params_, mnb_gsc.best_score_)\n",
    "\n",
    "x_dev = fast_text_dev_embeddings_df.values\n",
    "y_dev = labels\n",
    "\n",
    "y_pred = mnb_gsc.predict(x_dev)\n",
    "print(classification_report(y_dev, y_pred))\n",
    "print(f1_score(y_dev, y_pred))\n",
    "print(recall_score(y_dev, y_pred))\n",
    "print(precision_score(y_dev, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_f1(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall)/(precision + recall + K.epsilon())\n",
    "    \n",
    "    return f1_val\n",
    "\n",
    "x = fast_text_train_embeddings_df.values\n",
    "y = train_df['label'].values\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "x = mm.fit_transform(x)\n",
    "\n",
    "x = x[:,:,None]\n",
    "\n",
    "red_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", \n",
    "            factor=0.6,\n",
    "            patience=2, \n",
    "            min_lr=0.0001,\n",
    "            verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_109 (LSTM)             (None, 100, 64)           16896     \n",
      "                                                                 \n",
      " lstm_110 (LSTM)             (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " flatten_41 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,145\n",
      "Trainable params: 54,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(LSTM(64, input_shape = x.shape[1:], return_sequences = True))\n",
    "model_lstm.add(LSTM(64))\n",
    "\n",
    "model_lstm.add(Dense(64, activation = 'relu'))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics = [binary_f1])\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 12s 216ms/step - loss: 0.6941 - binary_f1: 0.4306 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 4s 146ms/step - loss: 0.6934 - binary_f1: 0.1073 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.6935 - binary_f1: 0.1820 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6934 - binary_f1: 0.0000e+00\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 0.6934 - binary_f1: 0.0000e+00 - val_loss: 0.6939 - val_binary_f1: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.6935 - binary_f1: 0.0000e+00 - val_loss: 0.6939 - val_binary_f1: 0.0000e+00 - lr: 6.0000e-04\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6932 - binary_f1: 0.2677\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 0.6932 - binary_f1: 0.2677 - val_loss: 0.6931 - val_binary_f1: 0.7007 - lr: 6.0000e-04\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 4s 149ms/step - loss: 0.6932 - binary_f1: 0.0165 - val_loss: 0.6932 - val_binary_f1: 0.0000e+00 - lr: 3.6000e-04\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6932 - binary_f1: 0.0000e+00\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "25/25 [==============================] - 4s 143ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6937 - val_binary_f1: 0.0000e+00 - lr: 3.6000e-04\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 0.6933 - binary_f1: 0.0000e+00 - val_loss: 0.6939 - val_binary_f1: 0.0000e+00 - lr: 2.1600e-04\n",
      "Epoch 10/20\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_f1: 0.0000e+00\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "25/25 [==============================] - 4s 147ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6937 - val_binary_f1: 0.0000e+00 - lr: 2.1600e-04\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6937 - val_binary_f1: 0.0000e+00 - lr: 1.2960e-04\n",
      "Epoch 12/20\n",
      "24/25 [===========================>..] - ETA: 0s - loss: 0.6932 - binary_f1: 0.0000e+00\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.2960e-04\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 4s 143ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 4s 142ms/step - loss: 0.6931 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 4s 146ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.6931 - binary_f1: 0.0000e+00 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 0.6931 - binary_f1: 0.0000e+00 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 0.6931 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 4s 141ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6934 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a50d75e0>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "model_lstm.fit(x, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=20,\n",
    "            validation_data=(x_dev, y_dev),\n",
    "            class_weight=class_weights,\n",
    "            shuffle=True, \n",
    "            callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 23ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       155\n",
      "           1       0.00      0.00      0.00       185\n",
      "\n",
      "    accuracy                           0.46       340\n",
      "   macro avg       0.23      0.50      0.31       340\n",
      "weighted avg       0.21      0.46      0.29       340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_lstm.predict(x_dev)\n",
    "y_pred_final = np.where(y_pred > 0.5, 1, 0)\n",
    "print(classification_report(y_dev, y_pred_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_45 (Bidirecti  (3075, 100, 128)         33792     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_46 (Bidirecti  (3075, 128)              98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_84 (Dense)            (3075, 64)                8256      \n",
      "                                                                 \n",
      " flatten_42 (Flatten)        (3075, 64)                0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (3075, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140,929\n",
      "Trainable params: 140,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bilstm  =  Sequential()\n",
    "\n",
    "model_bilstm.add(Bidirectional(LSTM(64, input_shape = x.shape[1:], return_sequences=True)))\n",
    "model_bilstm.add(Bidirectional(LSTM(64)))\n",
    "\n",
    "model_bilstm.add(Dense(64, activation = 'relu'))\n",
    "model_bilstm.add(Flatten())\n",
    "model_bilstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_bilstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics = [binary_f1])\n",
    "model_bilstm.build(input_shape=x.shape)\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 30s 367ms/step - loss: 0.6937 - binary_f1: 0.2487 - val_loss: 0.6932 - val_binary_f1: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.6940 - binary_f1: 0.5518 - val_loss: 0.6933 - val_binary_f1: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.6933 - binary_f1: 0.5505 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 0.6934 - binary_f1: 0.5543 - val_loss: 0.6930 - val_binary_f1: 0.7007 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6936 - binary_f1: 0.6938\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "25/25 [==============================] - 6s 259ms/step - loss: 0.6936 - binary_f1: 0.6938 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 7s 263ms/step - loss: 0.6932 - binary_f1: 0.5540 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 6.0000e-04\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6932 - binary_f1: 0.6632\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "25/25 [==============================] - 7s 263ms/step - loss: 0.6932 - binary_f1: 0.6632 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 6.0000e-04\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 6s 259ms/step - loss: 0.6932 - binary_f1: 0.0264 - val_loss: 0.6936 - val_binary_f1: 0.0000e+00 - lr: 3.6000e-04\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6933 - binary_f1: 0.0000e+00\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "25/25 [==============================] - 7s 263ms/step - loss: 0.6933 - binary_f1: 0.0000e+00 - val_loss: 0.6931 - val_binary_f1: 0.7007 - lr: 3.6000e-04\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 6s 259ms/step - loss: 0.6932 - binary_f1: 0.6582 - val_loss: 0.6929 - val_binary_f1: 0.7007 - lr: 2.1600e-04\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6932 - binary_f1: 0.1937\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.6932 - binary_f1: 0.1937 - val_loss: 0.6933 - val_binary_f1: 0.0000e+00 - lr: 2.1600e-04\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 6s 258ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.2960e-04\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6933 - binary_f1: 0.0000e+00\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.6933 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.2960e-04\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 7s 273ms/step - loss: 0.6933 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.6933 - binary_f1: 0.0000e+00 - val_loss: 0.6935 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 7s 278ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6933 - val_binary_f1: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 7s 283ms/step - loss: 0.6932 - binary_f1: 0.0000e+00 - val_loss: 0.6931 - val_binary_f1: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 0.6931 - binary_f1: 0.0175 - val_loss: 0.6930 - val_binary_f1: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 7s 266ms/step - loss: 0.6931 - binary_f1: 0.2798 - val_loss: 0.6930 - val_binary_f1: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 7s 267ms/step - loss: 0.6931 - binary_f1: 0.0456 - val_loss: 0.6931 - val_binary_f1: 0.7007 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a69097e0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "model_bilstm.fit(x, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=20,\n",
    "            validation_data=(x_dev, y_dev),\n",
    "            class_weight=class_weights,\n",
    "            shuffle=True,\n",
    "            callbacks=[red_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 3s 36ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       155\n",
      "           1       0.54      1.00      0.70       185\n",
      "\n",
      "    accuracy                           0.54       340\n",
      "   macro avg       0.27      0.50      0.35       340\n",
      "weighted avg       0.30      0.54      0.38       340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_bilstm.predict(x_dev)\n",
    "y_pred_final = np.where(y_pred > 0.5, 1, 0)\n",
    "print(classification_report(y_dev, y_pred_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CASE-cD2HfaeL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
